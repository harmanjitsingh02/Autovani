Abstract:
AutoVaani is an AI-powered car voice assistant developed to
provide a safe, intelligent, and hands-free driving experience
through natural language interaction. The project leverages
advanced speech recognition techniques and transformer-
based large language models to understand user commands
and respond conversationally. The system is designed using a
modular backend architecture implemented in Python with
Flask, enabling real-time voice-based interaction. AutoVaani
focuses on stability, scalability, and efficient CPU-based
execution, making it suitable for environments with limited
computational resources. This project highlights the practical
implementation of artificial intelligence, natural language
processing, and speech technologies in the automotive
domain.

Introduction:
The automotive industry is witnessing a significant
transformation with the integration of artificial intelligence and
smart technologies. Voice assistants have become an essential
feature in modern vehicles, allowing drivers to interact with
systems without manual intervention. However, many existing
voice-controlled systems rely on predefined commands and
lack conversational intelligence. AUToVaani aims to overcome
these limitations by introducing an AI-driven assistant capable
of understanding natural speech and generating context-aware
responses. The project demonstrates how AI can enhance
driver convenience, safety, and engagement by reducing
distractions and improving accessibility.

Problem Statement:
Despite the availability of voice-controlled systems in vehicles,
many suffer from limited understanding, poor accuracy, and
rigid command structures. These systems often fail when faced
with natural language variations, accents, or complex queries.
Additionally, deploying advanced AI models in real-time
environments presents challenges related to hardware
constraints, response latency, and system stability. The
problem addressed by AutoVaani is the development of an
intelligent, conversational, and lightweight voice assistant that
operates efficiently while providing accurate and meaningful
responses in an automotive setting.

Objective:
The objectives of the AutoVaani project are:
‚Ä¢ To design and develop an AI-based voice assistant for
automotive use
‚Ä¢ To enable seamless speech-to-text and text-to-speech
interaction
‚Ä¢ To implement a transformer-based language model for
intelligent response generation
‚Ä¢ To ensure system stability using CPU-based deployment
‚Ä¢ To create a scalable and modular backend architecture
‚Ä¢ To enhance user experience through natural and
conversational interaction

Scope of Project:
The scope of AutoVaani is limited to software-level
implementation and does not include direct hardware
integration with vehicle systems. The project serves as a proof-
of-concept for intelligent voice assistants in cars. It focuses on
backend processing, AI model integration, and API-based
communication. The system is designed to be extendable,
allowing future integration with navigation systems, vehicle
controls, and external APIs. The project also explores
multilingual and conversational AI capabilities within defined
constraints.

Agile methodology:
The project development followed the Agile methodology,
emphasizing iterative development and continuous
improvement. The work was divided into small development
cycles, each focusing on a specific feature such as speech
recognition, language model integration, or response handling.
Regular testing and mentor feedback played a crucial role in
refining system performance. This approach allowed flexibility
in adapting to challenges and ensured that the project evolved
efficiently toward its objectives.


Requirement Analysis:
Functional Requirements
‚Ä¢ Capture and process user voice input
‚Ä¢ Convert audio input into text using speech recognition
‚Ä¢ Interpret user intent using an AI language model
‚Ä¢ Generate context-aware textual responses
‚Ä¢ Convert text responses into speech output
‚Ä¢ Handle API requests and responses efficiently
Non-Functional Requirements
‚Ä¢ Low response latency for real-time interaction
‚Ä¢ High system reliability and fault tolerance
‚Ä¢ Compatibility with CPU-based environments
‚Ä¢ Modular and maintainable code structure
‚Ä¢ Secure handling of data and API endpoints



System Design:
The above diagram represents the end-to-end working
architecture of AutoVaani, a Hindi voice-based AI assistant.
The system enables a user to interact with the application using
spoken Hindi commands, processes the audio using AI and
NLP techniques on the backend, and returns an intelligent
spoken response.
The architecture follows a client‚Äìserver model, where the
frontend handles user interaction and audio capture, while the
Flask backend manages speech recognition, intent processing
using an LLM, and text-to-speech conversion.

1. User Interaction (Voice Input)
‚Ä¢ The process begins when the user speaks a
command in Hindi.
‚Ä¢ This could be a query such as navigation assistance,
system control, or general information.
‚Ä¢ The interaction is completely voice-driven, making
the system hands-free and user-friendly.

2. Browser (Audio Capture)
‚Ä¢ The browser acts as the first interface between the
user and the system.
‚Ä¢ Using browser-based APIs (such as Web Audio or
MediaRecorder), the user‚Äôs voice is recorded in real
time.
‚Ä¢ The recorded audio is temporarily stored and
prepared for transmission to the frontend application.

3. Frontend (Client Application)
‚Ä¢ The frontend receives the recorded audio from the
browser.
‚Ä¢ It acts as a communication bridge between the
browser and the backend.
‚Ä¢ The audio file is sent to the backend using an HTTP
POST request
‚Ä¢ This request includes the recorded audio data for
further processing.

4. Flask Backend (API Layer)
‚Ä¢ The Flask backend serves as the core processing
unit of the system.
‚Ä¢ It receives the audio input from the frontend and
coordinates all backend operations.
‚Ä¢ Flask ensures:
‚Ä¢ Request handling
‚Ä¢ Audio file management
‚Ä¢ Integration with AI models
‚Ä¢ Response generation in a structured JSON
format

5. Speech Recognition (Speech-to-Text)
‚Ä¢ The received audio is forwarded to the Speech
Recognition module.
‚Ä¢ This module converts the Hindi speech audio into
Hindi text using Speech-to-Text (STT) techniques.
‚Ä¢ The output of this step is a clean textual
representation of the user‚Äôs spoken command.
6. Intent Logic Processing
‚Ä¢ The recognized Hindi text is passed to the Intent
Logic module.
‚Ä¢ This module is responsible for:
‚Ä¢ Understanding the user‚Äôs intent
‚Ä¢ Classifying the command (e.g., control, query,
assistance)
‚Ä¢ Deciding how the system should respond
‚Ä¢ It uses rule-based logic combined with AI reasoning.

7. LLM (Large Language Model)
‚Ä¢ The LLM (Large Language Model) processes the
intent and context.
‚Ä¢ It generates a natural, human-like response in Hindi.
‚Ä¢ The LLM ensures:
‚Ä¢ Context awareness
‚Ä¢ Meaningful and accurate replies
‚Ä¢ Conversational tone suitable for a voice
assistant

8. Text-to-Speech Conversion
‚Ä¢ The generated Hindi text response is sent to the
Text-to-Speech (TTS) module.
‚Ä¢ This module converts the text into spoken audio.
‚Ä¢ The output is an MP3 audio file, making the response
audible to the user.
9. Flask Response Handling
‚Ä¢ Once the MP3 file is generated, the Flask backend
prepares the final response.
‚Ä¢ The backend sends:
‚Ä¢ A JSON response
‚Ä¢ An audio file URL pointing to the generated
speech
‚Ä¢ This structured response ensures easy handling on
the frontend.

10. Frontend Playback (Voice Output)
‚Ä¢ The frontend receives the JSON response from
Flask.
‚Ä¢ It extracts the audio URL and plays the MP3 file
automatically.
‚Ä¢ The user hears the AI assistant‚Äôs spoken response in
Hindi, completing the interaction cycle.


Codes:

app.py
from flask import Flask, request, jsonify
from flask_cors import CORS
import speech_recognition as sr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import os
import tempfile
import subprocess

# ---------------- Flask Setup ----------------
app = Flask(__name__)
CORS(app)
# ---------------- Model Setup ----------------
print("\nLoading Hindi car assistant model...", flush=True)
MODEL_PATH = "Qwen/Qwen2.5-0.5B-Instruct"
# ‚úÖ Force CPU for stability
device = "cpu"
dtype = torch.float32
print(f"Using device: {device}"
, flush=True)
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(
MODEL_PATH,
torch_dtype=dtype
).to(device)
pipe = pipeline(
"text-generation",
model=model,
tokenizer=tokenizer,
device=-1
)
alpaca_prompt_template = """You are a helpful car voice assistant.
Reply briefly in Hindi.
User command:
{}
Assistant response:
"""
# ---------------- Helpers ----------------
def ai_car_response(user_command: str) -> str:
prompt = alpaca_prompt_template.format(user_command)
outputs = pipe(
prompt,
max_new_tokens=80,
do_sample=True,
temperature=0.7,
top_p=0.9
)
text = outputs[0]["generated_text"]
return text.split("Assistant response:")[-1].strip()
def car_action_logic(user_command: str):
if "‡§ó‡§∞‡•ç‡§Æ‡•Ä" in user_command:
return "‡§†‡•Ä‡§ï ‡§π‡•à, ‡§∞‡•ç‡§Æ‡•à‡§Ç ‡§è‡§∏‡•Ä ‡§ö‡§æ‡§≤‡•Ç ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å‡•§"
if "‡§†‡§Ç ‡§°" in user_command:
return "‡§†‡•Ä‡§ï ‡§π‡•à, ‡§∞‡•ç‡§Æ‡•à‡§Ç ‡§è‡§∏‡•Ä ‡§¨‡§Ç‡§¶ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å‡•§"
if "‡§ñ‡§ø‡§°‡§º‡§ï‡•Ä" in user_command and "‡§ø‡•ã‡§≤" in user_command:
return "‡§†‡•Ä‡§ï ‡§π‡•à, ‡§∞‡•ç‡§Æ‡•à‡§Ç ‡§ñ‡§ø‡§°‡§º‡§ï‡•Ä ‡§ø‡•ã‡§≤ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å‡•§"
if "‡§ñ‡§ø‡§°‡§º‡§ï‡•Ä" in user_command and "‡§¨‡§Ç‡§¶" in user_command:
return "‡§†‡•Ä‡§ï ‡§π‡•à, ‡§∞‡•ç‡§Æ‡•à‡§Ç ‡§ñ‡§ø‡§°‡§º‡§ï‡•Ä ‡§¨‡§Ç‡§¶ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å‡•§"
return None
# ---------------- API ----------------
@app.route("/api/voice-command", methods=["POST"])
def handle_voice_command():
print("\nüì• Request received", flush=True)
webm_path = None
wav_path = None
try:
if "audio" not in request.files:
return jsonify({"error": "No audio file provided"}), 400
audio_file = request.files["audio"]
recognizer = sr.Recognizer()
# Save WebM audio
with tempfile.NamedTemporaryFile(delete=False, suffix=".webm") as tmp:
audio_file.save(tmp.name)
webm_path = tmp.name
wav_path = webm_path.replace(".webm", ".wav")
print("üîÑ Converting audio to WAV", flush=True)
# Convert WebM ‚Üí WAV (clean + louder)
subprocess.run(
[
"ffmpeg",
"
-y",
"
-i", webm_path,
"
-ac", "1",
"
-ar", "16000",
"
-af", "silenceremove=1:0:-50dB,volume=2",
wav_path
],
stdout=subprocess.DEVNULL,
stderr=subprocess.DEVNULL,
check=True
)
# Speech Recognition
with sr.AudioFile(wav_path) as source:
recognizer.adjust_for_ambient_noise(source, duration=0.6)
audio_data = recognizer.record(source)
print("üü° Calling Google Speech API", flush=True)
user_command_text = recognizer.recognize_google(
audio_data, language="hi-IN"
)
print("üß† User said:", user_command_text, flush=True)
# Decide response
rule_response = car_action_logic(user_command_text)
if rule_response:
car_response = rule_response
print("‚öôÔ∏è Rule-based response used", flush=True)
else:
print("ü§ñ Using LLM fallback", flush=True)
car_response = ai_car_response(user_command_text)
return jsonify({
"user_command": user_command_text,
"car_response": car_response
})
except sr.UnknownValueError:
return jsonify({
"error": "‡§∞‡•ç‡§Æ‡•à‡§Ç ‡§∏‡§∞‡•ç‡§Æ‡§ù ‡§®‡§π‡•Ä‡§Ç ‡§™‡§æ‡§Ø‡§æ‡•§ ‡§ï‡•É ‡§™‡§Ø‡§æ ‡§ß‡•Ä‡§∞‡•á ‡§î‡§∞ ‡§∏‡§æ‡§´‡§º ‡§¨‡•ã‡§≤‡•á‡§Ç‡•§"
}), 400
except sr.RequestError as e:
print("‚ùå Google Speech API error:", e, flush=True)
return jsonify({
"error": "Speech service unavailable"
}), 503
except Exception as e:
print("üî• ERROR:", repr(e), flush=True)
return jsonify({"error": str(e)}), 500
finally:
for f in [webm_path, wav_path]:
if f and os.path.exists(f):
os.remove(f)
print("üßπ Temp files cleaned", flush=True)
# ---------------- Run ----------------
if __name__ == "__main__":
app.run(
host="0.0.0.0",
port=5001,
debug=False,
use_reloader=False
)
Index.html
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Hindi Car Voice Assistant</title>
<style>
body {
font-family: Arial, sans-serif;
background: #111;
color: #fff;
text-align: center;
padding: 40px;
}
button {
padding: 15px 25px;
font-size: 18px;
cursor: pointer;
border: none;
border-radius: 8px;
background: #00c853;
color: white;
}
button:disabled {
background: #555;
}
.box {
margin-top: 20px;
padding: 15px;
background: #222;
border-radius: 8px;
text-align: left;
max-width: 600px;
margin: auto;
}
.label {
color: #00e5ff;
font-weight: bold;
}
.hint {
margin-top: 10px;
color: #aaa;
font-size: 14px;
}
</style>
</head>
<body>
<h1>üöó Hindi Car Voice Assistant</h1>
<button id="recordBtn">üéôÔ∏è Start Speaking</button>
<p class="hint">Click ‚Üí wait 1 second ‚Üí speak slowly and clearly in Hindi</p>
<div class="box">
<p><span class="label">You said:</span> <span id="userText">---</span></p>
<p><span class="label">Car response:</span> <span id="carText">---
</span></p>
</div>
<audio id="carAudio" autoplay></audio>
<script>
let mediaRecorder;
let audioChunks = [];
let stream;
const recordBtn = document.getElementById("recordBtn");
const carAudio = document.getElementById("carAudio");
recordBtn.onclick = async () => {
audioChunks = [];
recordBtn.disabled = true;
recordBtn.innerText =
"üéôÔ∏è Listening...";
// üé§ Request microphone with proper constraints
stream = await navigator.mediaDevices.getUserMedia({
audio: {
echoCancellation: true,
noiseSuppression: true,
autoGainControl: true
}
});
// ‚úÖ Force a stable mime type
mediaRecorder = new MediaRecorder(stream, {
mimeType: "audio/webm;codecs=opus"
});
mediaRecorder.start();
mediaRecorder.ondataavailable = e => {
if (e.data.size > 0) audioChunks.push(e.data);
};
// ‚úÖ Give user enough time to speak
setTimeout(() => {
mediaRecorder.stop();
recordBtn.innerText =
"‚è≥ Processing...";
}, 7000);
mediaRecorder.onstop = async () => {
// üõë Stop mic immediately
stream.getTracks().forEach(track => track.stop());
const audioBlob = new Blob(audioChunks, { type: "audio/webm" });
const formData = new FormData();
formData.append("audio", audioBlob, "voice.webm");
try {
const res = await fetch("http://127.0.0.1:5001/api/voice-command",
{
method: "POST",
body: formData
});
const data = await res.json();
if (data.error) {
alert(data.error);
} else {
document.getElementById("userText").innerText =
data.user_command || "
‚Äî
";
document.getElementById("carText").innerText =
data.car_response || "
‚Äî
";
// üîä Play car response audio (if available)
if (data.audio_url) {
carAudio.src = "http://127.0.0.1:5001" + data.audio_url;
carAudio.play();
}
}
} catch (err) {
alert("Backend not reachable");
}
recordBtn.disabled = false;
recordBtn.innerText =
"üéôÔ∏è Start Speaking";
};
};
</script>
</body>
</html>



Implementation:
The implementation of AutoVaani is carried out using Python
due to its strong ecosystem for AI and backend development.
Flask is used to develop RESTful APIs that handle incoming
audio data and return responses. The SpeechRecognition
library processes real-time voice input, while the Hugging Face
Transformers library loads and executes the Qwen instruction-
tuned language model. The model is configured to run on CPU
to ensure stability and broader compatibility. Proper exception
handling, temporary file management, and resource
optimization techniques are implemented to enhance system
performance.

Tools & Technologies:
‚Ä¢ Python: Core programming language
‚Ä¢ Flask: Backend web framework
‚Ä¢ Hugging Face Transformers: Language model integration
‚Ä¢ PyTorch: Model execution and tensor operations
‚Ä¢ SpeechRecognition: Speech-to-text conversion
‚Ä¢ Flask-CORS: Cross-origin request handling
‚Ä¢ Git & GitHub: Version control and collaboration
‚Ä¢ VS Code / Google Colab: Development environment

Testing:
Testing was conducted at various levels to ensure accuracy
and reliability. Unit testing focused on validating individual
modules such as speech recognition and text generation.
Integration testing ensured smooth data flow between system
components. Real-world testing involved multiple voice inputs
with varying speech patterns to evaluate performance. The
system was also tested for edge cases such as unclear audio
input, silence, and network interruptions.

Deployment:
The AutoVaani backend is deployed as a Flask application that
can run locally or on cloud-based servers. The language model
is loaded during application startup to minimize runtime delays.
The application supports cross-origin requests, enabling
seamless integration with frontend interfaces. Deployment
considerations include performance optimization, ease of
setup, and scalability.

Maintenance:
Maintenance of AutoVaani is simplified due to its modular
architecture. Updates to individual components such as the AI
model or speech engine can be performed without affecting the
entire system. Regular monitoring and logging help identify
performance issues. Maintenance activities also include
dependency updates, performance tuning, and system security
checks.

Advantages:
‚Ä¢ Enables hands-free vehicle interaction
‚Ä¢ Improves driving safety by reducing distractions
‚Ä¢ Provides natural and conversational responses
‚Ä¢ Efficient CPU-based execution
‚Ä¢ Scalable and modular system design

Limitations:
‚Ä¢ Accuracy depends on speech clarity and background
noise
‚Ä¢ CPU-based inference may introduce latency
‚Ä¢ Requires internet access for model loading
‚Ä¢ Limited to software-level vehicle interaction
Future Enhancements:
‚Ä¢ Integration with navigation and vehicle control systems
‚Ä¢ Support for additional languages and accents
‚Ä¢ Offline AI model optimization
‚Ä¢ User personalization and voice profiles
‚Ä¢ Integration with IoT and smart vehicle ecosystems

Conclusion:
AutoVaani demonstrates the effective application of artificial
intelligence in the automotive domain by delivering an
intelligent and conversational voice assistant. The project
successfully integrates speech processing and transformer-
based language models into a scalable backend system. It
highlights the feasibility of deploying AI-powered solutions even
in resource-constrained environments. Overall, this project has
provided valuable hands-on experience in AI development,
system design, and real-world problem solving.

References:
‚Ä¢ Hugging Face Transformers Documentation
‚Ä¢ Flask Official Documentation
‚Ä¢ PyTorch Documentation
‚Ä¢ SpeechRecognition Library Documentation
‚Ä¢ Research articles on AI-based automotive voice assistants